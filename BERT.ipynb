{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49a7014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import TFBertModel\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34787b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload the TwIT Dataset\n",
    "twit=pd.read_csv(r\"C:\\Users\\ridol\\OneDrive\\Desktop\\Emotion Recognition\\Dataset\\TwIT.csv\",\n",
    "                 sep=';',header=0, encoding='utf8', dtype={'Text':'str','Emozione':'int'})\n",
    "\n",
    "twit.set_index('Id',drop=True,inplace=True) #I set as row index the id\n",
    "\n",
    "twit.rename(mapper={'Emozione':'Emotion'},axis='columns',inplace=True) #rename the colums 'Emozione' in 'Emotion'\n",
    "\n",
    "##Data Exploration\n",
    "twit.info()\n",
    "print(twit.isnull().sum()) #there is no null value\n",
    "freq = twit.groupby(['Emotion']).count()\n",
    "print(freq)\n",
    "#Result --> Happiness(0): 549;  Trust(1):504; Sadness(2):479; Anger(3):513; Fear(4):518; Disgust(5):545;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deefb619",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and Test split\n",
    "twit_shuffled=shuffle(twit)\n",
    "Training_set= twit_shuffled.iloc[623:] #80%\n",
    "Testing_set= twit_shuffled.iloc[:623] #20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54400496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\") #download From Huggingface library\n",
    "\n",
    "#Create the input id and attention mask\n",
    "X_input_ids = np.zeros((len(Training_set), 64))\n",
    "X_attn_masks = np.zeros((len(Training_set), 64))\n",
    "X_input_ids_test = np.zeros((len(Testing_set), 64))\n",
    "X_attn_masks_test = np.zeros((len(Testing_set), 64))\n",
    "\n",
    "\n",
    "#Create the function for training data\n",
    "def generate_training_data(df, ids, masks, tokenizer):\n",
    "    for i, text in tqdm(enumerate(Training_set['Text'])):\n",
    "        tokenized_text = tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=64,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        ids[i, :] = tokenized_text.input_ids\n",
    "        masks[i, :] = tokenized_text.attention_mask\n",
    "    return ids, masks\n",
    "\n",
    "X_input_ids, X_attn_masks = generate_training_data(Training_set, X_input_ids, X_attn_masks, tokenizer)\n",
    "\n",
    "#one hot encoding about labels\n",
    "labels= np.zeros((len(Training_set), 6))\n",
    "labels[np.arange(len(Training_set)), Training_set['Emotion'].values] = 1   #one-hot encoded target tensor\n",
    "\n",
    "#creating a data pipeline using tensorflow dataset utility, creates batches of data for easy loading\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))\n",
    "\n",
    "#create the map function\n",
    "def SentimentDatasetMapFunction(input_ids, attn_masks, labels):\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attn_masks\n",
    "    }, labels\n",
    "\n",
    "dataset = dataset.map(SentimentDatasetMapFunction)\n",
    "\n",
    "\n",
    "# batch size, drop any left out tensor\n",
    "dataset = dataset.shuffle(1000).batch(8, drop_remainder=True)\n",
    "\n",
    "p =0.8\n",
    "train_size = int((len(Training_set)//8)*p) #divide the dataset into 16 batch and for each batch we will take the 80% of data\n",
    "\n",
    "#Train and validation set\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e8c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT model\n",
    "bert_model= TFBertModel.from_pretrained('bert-base-multilingual-cased') # bert base model with pretrained weights\n",
    "\n",
    "# defining 2 input layers for input_ids and attn_masks\n",
    "input_ids = tf.keras.layers.Input(shape=(64,), name='input_ids', dtype='int32')\n",
    "attention_masks = tf.keras.layers.Input(shape=(64,), name='attention_mask', dtype='int32')\n",
    "\n",
    "bert_embds = bert_model.bert(input_ids, attention_mask=attention_masks)[1] # 0 -> activation layer (3D), 1 -> pooled output layer (2D)\n",
    "intermediate_layer = tf.keras.layers.Dense(64, activation='relu', name='intermediate_layer')(bert_embds)\n",
    "intermediate_layer = Dropout(0.2)(intermediate_layer)\n",
    "output_layer = tf.keras.layers.Dense(6, activation='softmax', name='output_layer')(intermediate_layer) # softmax -> calcs probs of classes\n",
    "\n",
    "\n",
    "emotional_model = tf.keras.Model(inputs=[input_ids, attention_masks], outputs=output_layer)\n",
    "emotional_model.summary()\n",
    "\n",
    "#optimizer, loss-function and accuracy of the emotional model\n",
    "optim = tf.keras.optimizers.Adam(learning_rate=2e-5, decay=1e-6)\n",
    "loss_func = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "emotional_model.compile(optimizer=optim, loss=loss_func, metrics=[acc])\n",
    "\n",
    "\n",
    "\n",
    "#Training the model\n",
    "###### ALERT ##### the following code can request good CPU or GPU. otherwise it will take a long time to train the model\n",
    "\n",
    "hist = emotional_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data= val_dataset,\n",
    "    epochs=6  #8-10 epochs are better but need a good GPU\n",
    ")\n",
    "\n",
    "emotional_model.save('emotional_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2d2d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction\n",
    "emotional_model = tf.keras.models.load_model('emotional_model')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "#Create the dictionary for test set\n",
    "dictionary_test_set = dict([(k,v) for k,v in zip(Testing_set['Text'], Testing_set['Emotion'])])\n",
    "y_true = list(dictionary_test_set.values())\n",
    "y_true =list(y_true)\n",
    "\n",
    "y_predict=[]\n",
    "for k in dictionary_test_set:\n",
    "\n",
    "    def prepare_data(input_text, tokenizer):\n",
    "        token = tokenizer.encode_plus(\n",
    "            input_text,\n",
    "            max_length=64,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': tf.cast(token.input_ids, tf.float64),\n",
    "            'attention_mask': tf.cast(token.attention_mask, tf.float64)\n",
    "        }\n",
    "\n",
    "    tokenized_input_text= prepare_data(k, tokenizer)\n",
    "    probs= emotional_model.predict(tokenized_input_text)\n",
    "    y_predict.append(np.argmax(probs[0]))\n",
    "    print(k, np.argmax(probs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfecf961",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the Classification report\n",
    "print('\\tClassification Report for BERT:\\n\\n',classification_report(y_true,y_predict,target_names=['Happiness', 'Trust', 'Sadness','Anger','Fear','Disgust']))\n",
    "\n",
    "#Print Confusion Matrix\n",
    "labels_6 = ['Happiness','Trust','Sadness','Anger','Fear','Disgust']\n",
    "def print_cf1(y_test, y_hat):\n",
    "    cm = confusion_matrix(y_test, y_hat)\n",
    "    sns.set(font_scale= 1.4, color_codes=True, palette=\"deep\")\n",
    "    sns.heatmap(pd.DataFrame(cm, index=labels_6, columns=[0,1,2,3,4,5]),\n",
    "                annot = True,\n",
    "                annot_kws = {\"size\":16},\n",
    "                fmt=\"d\",\n",
    "                cmap=\"YlGnBu\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted Value\")\n",
    "    plt.xticks([0,1,2,3,4,5], labels_6, rotation=45)\n",
    "    plt.ylabel(\"True Value\")\n",
    "    plt.show()\n",
    "\n",
    "print_cf1(y_true, y_predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c9a5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "import re\n",
    "import simplemma\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a578b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload the TwIT Dataset\n",
    "twit=pd.read_csv(r\"C:\\Users\\ridol\\OneDrive\\Desktop\\Emotion Recognition\\Dataset\\TwIT.csv\",\n",
    "                 sep=';',header=0, encoding='utf8', dtype={'Text':'str','Emozione':'str'})\n",
    "\n",
    "twit.set_index('Id',drop=True,inplace=True)  #I set as row index the id\n",
    "\n",
    "twit.rename(mapper={'Emozione':'Emotion'},axis='columns',inplace=True) #rename the colums 'Emozione' in 'Emotion'\n",
    "\n",
    "\n",
    "##Data Exploration\n",
    "twit.info()\n",
    "print(twit.isnull().sum()) #there is no null value\n",
    "freq = twit.groupby(['Emotion']).count()\n",
    "print(freq)\n",
    "#Result --> Happiness(0): 549;  Trust(1):504; Sadness(2):479; Anger(3):513; Fear(4):518; Disgust(5):545;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553d4bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing data\n",
    "twit_text=twit['Text']\n",
    "twit_label=twit['Emotion']\n",
    "it_stop = set(stopwords.words('italian'))\n",
    "\n",
    "\n",
    "def preprocessing(text):\n",
    "\n",
    "    # Preprocessing of raw text\n",
    "    text = re.sub(r'\\W',' ',str(text)) # Remove all the special characters\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)  # remove all single characters\n",
    "    text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', text)  # Remove single characters from the start\n",
    "    text = re.sub(r'\\s+',' ',text, flags=re.I)  # Substituting multiple spaces with single space\n",
    "    text = text.lower()   # Converting to Lowercase\n",
    "    text = re.sub(r'\\d',' ',text) #Removing numbers from strings\n",
    "    text = re.sub(r'^b\\s+', '', text) # Removing prefixed 'b' from byte string\n",
    "\n",
    "    #remove stop words and words less that 3 character\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in it_stop]\n",
    "    tokens = [word for word in tokens if len(word) > 3]\n",
    "    tokens = [word for word in tokens if word in wwv]\n",
    "\n",
    "    #Lemmatization\n",
    "    preprocessed_text = []\n",
    "    for word in tokens:\n",
    "        preprocessed_text.append(simplemma.lemmatize(word, lang=('it', 'en')))\n",
    "        \n",
    "    return preprocessed_text\n",
    "\n",
    "preprocessing(twit_text)\n",
    "\n",
    "##tokenize text\n",
    "twit_text_tokenized = [preprocessing(sentence) for sentence in twit_text if sentence.strip() !='']\n",
    "print(twit_text_tokenized)\n",
    "\n",
    "\n",
    "######## ALTERNATIVE METHODS #######\n",
    "##Alternative Preprocessing with Gensim library\n",
    "twit_text= twit_text.apply(gensim.utils.simple_preprocess) #library to do preprocessing. easy, but less accurate!!\n",
    "twit_text= [word for word in twit_text if word not in stopwords.words('italian')]\n",
    "#twit_text= [word for word in twit_text if word not in get_stop_words('italian')] #with stop-words package\n",
    "print(twit_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c054261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec model\n",
    "model= Word2Vec(sentences=twit_text_tokenized,\n",
    "               vector_size=128,\n",
    "               window=10,\n",
    "               min_count=1,\n",
    "               sg=0,\n",
    "               hs=1,\n",
    "               negative=5,\n",
    "               epochs=10,\n",
    "               sample=1e-3)\n",
    "\n",
    "\n",
    "model.build_vocab(twit_text_tokenized, progress_per=3000)\n",
    "\n",
    "model.train(twit_text_tokenized, total_examples=model.corpus_count, epochs=10)\n",
    "\n",
    "#Print some results\n",
    "print(model.wv.most_similar('buongiorno',topn=3))\n",
    "print(model.wv.most_similar('dispiacere',topn=3))\n",
    "print(model.wv.most_similar('vomitare',topn=3))\n",
    "(model.wv.similarity(w1='bravo', w2='complimento'))\n",
    "print(model.wv.doesnt_match(['ospedale','felicit√†','amore'])) #Returns the word that has nothing to do with anything\n",
    "vector = model.wv['computer'] #see the vector of each word\n",
    "print(vector)\n",
    "print(model.predict_output_word(['ti','amo','amore'], topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d73c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Embedding Visualization\n",
    "vocab= list(model.wv.key_to_index)\n",
    "X = model.wv[vocab]\n",
    "\n",
    "#### You can use TSNE or PCA\n",
    "\n",
    "#TSNE: T-distribuited Stochastics Neighbor Embedding\n",
    "# with tsne to reduce 2 dimensons\n",
    "tsne = TSNE(n_components=2,metric='cosine')\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "df = pd.DataFrame(X_tsne, index=vocab) #columns=['x', 'y'])\n",
    "\n",
    "\n",
    "#PCA:Principal Component Analysis\n",
    "#with pca to reduce 2 dimensions\n",
    "pca=PCA(n_components=50)\n",
    "X_pca=pca.fit_transform(X)\n",
    "df1=pd.DataFrame(X_pca,index=vocab) #columns=['x','y'])\n",
    "\n",
    "#### You can use MATPLOTLIB or PLOTY for visualize the word embedding in 2 dimension space\n",
    "\n",
    "##Plot the words with matplotlib\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(df['x'], df['y'])\n",
    "\n",
    "for word, pos in df.iterrows():\n",
    "    ax.annotate(word, pos)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "##plot the words with plotly\n",
    "fig=px.scatter(df1, x='x', y='y',text=vocab,log_x=True, size_max=500)\n",
    "fig.update_traces(textposition='top center')\n",
    "fig.update_layout(height=500,\n",
    "                  title_text='Word embedding chart')\n",
    "fig.write_html('Word Embedding.html', auto_open=True )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0412736a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model in csv file\n",
    "vocabolary,vectors = model.wv.key_to_index, model.wv.vectors\n",
    "# get node name and embedding vector index.\n",
    "name_index = np.array([(v[0], v[1]) for v in vocabolary.items()])\n",
    "# init dataframe using embedding vectors and set index as node name\n",
    "df = pd.DataFrame(vectors[name_index[:, 1].astype(int)])\n",
    "df.index = name_index[:, 0]\n",
    "df.to_csv(\"word_embedding.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
